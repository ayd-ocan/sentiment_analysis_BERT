{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ocanaydin/financial-sentiment-bert?scriptVersionId=123626002\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-27T20:11:38.397299Z","iopub.execute_input":"2023-03-27T20:11:38.397707Z","iopub.status.idle":"2023-03-27T20:11:38.415594Z","shell.execute_reply.started":"2023-03-27T20:11:38.397671Z","shell.execute_reply":"2023-03-27T20:11:38.414399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DATA PREPROCESSING","metadata":{}},{"cell_type":"code","source":"#Fetch data from dataset.\ndf = pd.read_csv(\"/kaggle/input/financial-sentiment-analysis/data.csv\")\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-03-27T20:50:36.562836Z","iopub.execute_input":"2023-03-27T20:50:36.5632Z","iopub.status.idle":"2023-03-27T20:50:36.597783Z","shell.execute_reply.started":"2023-03-27T20:50:36.563169Z","shell.execute_reply":"2023-03-27T20:50:36.596822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Negative : {sum(df['Sentiment'] == 'negative')}\")\nprint(f\"Neutral : {sum(df['Sentiment'] == 'neutral')}\")\nprint(f\"Positive : {sum(df['Sentiment'] == 'positive')}\")","metadata":{"execution":{"iopub.status.busy":"2023-03-27T20:53:38.14716Z","iopub.execute_input":"2023-03-27T20:53:38.147739Z","iopub.status.idle":"2023-03-27T20:53:38.175Z","shell.execute_reply.started":"2023-03-27T20:53:38.147691Z","shell.execute_reply":"2023-03-27T20:53:38.173557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check if any null value in dataset.\nprint(df.isnull().sum())\n#We see that there is no null value so no need to drop any row.\nprint(len(df))","metadata":{"execution":{"iopub.status.busy":"2023-03-27T20:11:49.163737Z","iopub.execute_input":"2023-03-27T20:11:49.164387Z","iopub.status.idle":"2023-03-27T20:11:49.171979Z","shell.execute_reply.started":"2023-03-27T20:11:49.164349Z","shell.execute_reply":"2023-03-27T20:11:49.170818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Some libraries\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-03-27T20:11:51.62241Z","iopub.execute_input":"2023-03-27T20:11:51.622778Z","iopub.status.idle":"2023-03-27T20:12:00.177256Z","shell.execute_reply.started":"2023-03-27T20:11:51.622746Z","shell.execute_reply":"2023-03-27T20:12:00.176205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Apply One hot encoder to convert sentiment column to every seperate column(negative,neutral,positive.) \nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\n\nencoded_y = encoder.fit_transform(df[\"Sentiment\"])\ndummy_y = tf.keras.utils.to_categorical(encoded_y)","metadata":{"execution":{"iopub.status.busy":"2023-03-27T20:12:03.553423Z","iopub.execute_input":"2023-03-27T20:12:03.554132Z","iopub.status.idle":"2023-03-27T20:12:03.802242Z","shell.execute_reply.started":"2023-03-27T20:12:03.554086Z","shell.execute_reply":"2023-03-27T20:12:03.801216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Convert array to dataframe and concatenate two dataframes.\ndf_sentiment = pd.DataFrame(dummy_y,columns = encoder.classes_,index = df[\"Sentence\"].index)","metadata":{"execution":{"iopub.status.busy":"2023-03-27T20:12:08.635981Z","iopub.execute_input":"2023-03-27T20:12:08.636371Z","iopub.status.idle":"2023-03-27T20:12:08.643201Z","shell.execute_reply.started":"2023-03-27T20:12:08.636337Z","shell.execute_reply":"2023-03-27T20:12:08.641796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Drop sentiment column to change it to its one hot encoded version.\ndf.drop(columns = [\"Sentiment\"],inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-03-27T20:12:09.152162Z","iopub.execute_input":"2023-03-27T20:12:09.152569Z","iopub.status.idle":"2023-03-27T20:12:09.160799Z","shell.execute_reply.started":"2023-03-27T20:12:09.15253Z","shell.execute_reply":"2023-03-27T20:12:09.159553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([df,df_sentiment],axis = 1)\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-03-27T20:12:12.100792Z","iopub.execute_input":"2023-03-27T20:12:12.101355Z","iopub.status.idle":"2023-03-27T20:12:12.12875Z","shell.execute_reply.started":"2023-03-27T20:12:12.101305Z","shell.execute_reply":"2023-03-27T20:12:12.126878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Split dataset as train and test.\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(df[\"Sentence\"],df.iloc[:,1:],test_size = 0.5,\n                                                 random_state = 42,shuffle = False)","metadata":{"execution":{"iopub.status.busy":"2023-03-27T20:12:15.985135Z","iopub.execute_input":"2023-03-27T20:12:15.985512Z","iopub.status.idle":"2023-03-27T20:12:16.051436Z","shell.execute_reply.started":"2023-03-27T20:12:15.985456Z","shell.execute_reply":"2023-03-27T20:12:16.050513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create validation dataset from test dataset.\nX_test,X_val,y_test,y_val = train_test_split(X_test,y_test,test_size = 0.5,random_state = 42,shuffle = False)","metadata":{"execution":{"iopub.status.busy":"2023-03-27T20:12:27.108541Z","iopub.execute_input":"2023-03-27T20:12:27.109312Z","iopub.status.idle":"2023-03-27T20:12:27.116527Z","shell.execute_reply.started":"2023-03-27T20:12:27.109275Z","shell.execute_reply":"2023-03-27T20:12:27.115355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CHOOSE A BERT MODEL TO FINE TUNE","metadata":{}},{"cell_type":"code","source":"#You can pick your BERT MODEL from tensorflow hub. \ntfhub_handle_encoder = \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/2\"\ntfhub_handle_preprocess = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n\nprint(f'BERT model selected           : {tfhub_handle_encoder}')\nprint(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')","metadata":{"execution":{"iopub.status.busy":"2023-03-27T20:12:27.914963Z","iopub.execute_input":"2023-03-27T20:12:27.91594Z","iopub.status.idle":"2023-03-27T20:12:27.922268Z","shell.execute_reply.started":"2023-03-27T20:12:27.915893Z","shell.execute_reply":"2023-03-27T20:12:27.92113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Define preprocess model to tokenize words with respect to BERT.\nbert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)","metadata":{"execution":{"iopub.status.busy":"2023-03-27T20:12:31.796667Z","iopub.execute_input":"2023-03-27T20:12:31.797036Z","iopub.status.idle":"2023-03-27T20:12:37.089198Z","shell.execute_reply.started":"2023-03-27T20:12:31.797002Z","shell.execute_reply":"2023-03-27T20:12:37.088142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#See example about tokenizer.Notice => Bert model takes input as list.\nidx = X_train.index[10]\nex = [X_train[idx]]\nex_preprocessed = bert_preprocess_model(ex)\nprint(f\"Sentence : {ex}\")\nprint(f'Keys       : {list(ex_preprocessed.keys())}')\nprint(f'Shape      : {ex_preprocessed[\"input_word_ids\"].shape}')\nprint(f'Word Ids   : {ex_preprocessed[\"input_word_ids\"][0, :20]}')\nprint(f'Input Mask : {ex_preprocessed[\"input_mask\"][0, :20]}')\nprint(f'Type Ids   : {ex_preprocessed[\"input_type_ids\"][0, :20]}')","metadata":{"execution":{"iopub.status.busy":"2023-03-27T20:12:45.748095Z","iopub.execute_input":"2023-03-27T20:12:45.748694Z","iopub.status.idle":"2023-03-27T20:12:46.045433Z","shell.execute_reply.started":"2023-03-27T20:12:45.748649Z","shell.execute_reply":"2023-03-27T20:12:46.044304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**DEFINE YOUR MODEL**","metadata":{}},{"cell_type":"code","source":"def build_classifier_model():\n    #Get text input and preprocess it.\n    text_input = tf.keras.layers.Input(shape = (),dtype = tf.string,name = \"text\")\n    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess,name = \"preprocessing\")\n    encoder_inputs = preprocessing_layer(text_input)\n    #Use encoder to apply BERT MODEL.\n    encoder = hub.KerasLayer(tfhub_handle_encoder,trainable = True,name = \"BERT_encoder\")\n    outputs = encoder(encoder_inputs)\n    #You can think of a pooled_output as a embedding for the entire movie_review.\n    net = outputs[\"pooled_output\"]\n    #Droupout and output layers.\n    net = tf.keras.layers.Dropout(0.25)(net)\n    net = tf.keras.layers.Dense(3,activation = \"softmax\",name = \"classifier\")(net)\n    return tf.keras.Model(text_input,net)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-03-27T20:14:05.336441Z","iopub.execute_input":"2023-03-27T20:14:05.336898Z","iopub.status.idle":"2023-03-27T20:14:05.346034Z","shell.execute_reply.started":"2023-03-27T20:14:05.336859Z","shell.execute_reply":"2023-03-27T20:14:05.344796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Assign classifer model see the model on graph.\nmodel = build_classifier_model()\ntf.keras.utils.plot_model(model)","metadata":{"execution":{"iopub.status.busy":"2023-03-27T20:14:09.701131Z","iopub.execute_input":"2023-03-27T20:14:09.701524Z","iopub.status.idle":"2023-03-27T20:14:23.76565Z","shell.execute_reply.started":"2023-03-27T20:14:09.701481Z","shell.execute_reply":"2023-03-27T20:14:23.764494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MODEL TRAINING","metadata":{}},{"cell_type":"code","source":"#Optimizer\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n#Compile the model\nmodel.compile(optimizer = optimizer,loss = \"categorical_crossentropy\",metrics = \"accuracy\") ","metadata":{"execution":{"iopub.status.busy":"2023-03-27T20:14:26.027358Z","iopub.execute_input":"2023-03-27T20:14:26.028426Z","iopub.status.idle":"2023-03-27T20:14:26.055398Z","shell.execute_reply.started":"2023-03-27T20:14:26.028382Z","shell.execute_reply":"2023-03-27T20:14:26.054413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Training model : {tfhub_handle_encoder}\")\nhistory = model.fit(x = X_train,y = y_train,validation_data = (X_val,y_val),epochs = 15,batch_size = 128)","metadata":{"execution":{"iopub.status.busy":"2023-03-27T20:18:35.938094Z","iopub.execute_input":"2023-03-27T20:18:35.938534Z","iopub.status.idle":"2023-03-27T20:30:16.942768Z","shell.execute_reply.started":"2023-03-27T20:18:35.938495Z","shell.execute_reply":"2023-03-27T20:30:16.941709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss,accuracy = model.evaluate(X_test,y_test)\nprint(f\"Loss : {loss} , acc : {accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2023-03-27T20:30:28.795094Z","iopub.execute_input":"2023-03-27T20:30:28.795454Z","iopub.status.idle":"2023-03-27T20:30:39.09111Z","shell.execute_reply.started":"2023-03-27T20:30:28.795422Z","shell.execute_reply":"2023-03-27T20:30:39.090009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PLOT HISTORY","metadata":{}},{"cell_type":"code","source":"history = history \n#Metrics\nacc = history[\"accuracy\"]\nloss = history[\"loss\"]\nval_acc = history[\"val_accuracy\"]\nval_loss = history[\"val_loss\"]\n\nepochs = range(1,len(acc) + 1)\n#Plot Accuracy\nfig = plt.figure(figsize = (10,6))\nplt.subplot(2,1,1)\nplt.plot(epochs,acc,\"r\",label = \"Training acc\")\nplt.plot(epochs,val_acc,\"b\",label = \"Validation acc\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\n#Plot Loss\nplt.subplot(2,1,2)\nplt.plot(epochs,loss,\"r\",label = \"Training loss\")\nplt.plot(epochs,val_loss,\"b\",label = \"Validation loss\")\nplt.ylabel(\"Loss\")\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-27T20:31:38.982003Z","iopub.execute_input":"2023-03-27T20:31:38.982428Z","iopub.status.idle":"2023-03-27T20:31:39.309728Z","shell.execute_reply.started":"2023-03-27T20:31:38.982392Z","shell.execute_reply":"2023-03-27T20:31:39.308751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SAVE MODEL","metadata":{}},{"cell_type":"code","source":"model.save(\"financial_sentiment_BERT\")","metadata":{"execution":{"iopub.status.busy":"2023-03-27T20:49:19.690974Z","iopub.execute_input":"2023-03-27T20:49:19.691343Z","iopub.status.idle":"2023-03-27T20:49:34.634518Z","shell.execute_reply.started":"2023-03-27T20:49:19.69131Z","shell.execute_reply":"2023-03-27T20:49:34.63351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_saved = tf.saved_model.load(\"financial_sentiment_BERT\")","metadata":{"execution":{"iopub.status.busy":"2023-03-27T20:49:37.159932Z","iopub.execute_input":"2023-03-27T20:49:37.160305Z","iopub.status.idle":"2023-03-27T20:49:49.354896Z","shell.execute_reply.started":"2023-03-27T20:49:37.160272Z","shell.execute_reply":"2023-03-27T20:49:49.353854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PREDICTION","metadata":{}},{"cell_type":"code","source":"#(1)Get text example from test dataset.\nn1 = 20\nn2 = 30\n\ntest_ex = X_test[n1:n2]\npreds = model.predict([test_ex])\n#Find biggest element in array to predict class.\npreds = np.argmax(preds,axis = 1) \nclasses = encoder.classes_\n\nfor i in range(len(preds)):\n    real = np.argmax(y_test[n1 + i : n1+ 1 + i])\n    print(f\"Prediction : {classes[preds[i]]} Real : {classes[real]}\")","metadata":{"execution":{"iopub.status.busy":"2023-03-27T20:49:59.968441Z","iopub.execute_input":"2023-03-27T20:49:59.969424Z","iopub.status.idle":"2023-03-27T20:50:00.073413Z","shell.execute_reply.started":"2023-03-27T20:49:59.969387Z","shell.execute_reply":"2023-03-27T20:50:00.072507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#These sentences are test sentences which are predicted above by the model.\nX_test[n1 : n2]","metadata":{"execution":{"iopub.status.busy":"2023-03-27T20:50:07.361178Z","iopub.execute_input":"2023-03-27T20:50:07.361559Z","iopub.status.idle":"2023-03-27T20:50:07.371791Z","shell.execute_reply.started":"2023-03-27T20:50:07.361526Z","shell.execute_reply":"2023-03-27T20:50:07.370715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#(2) You can write your own sentence and predict its sentiment.\nown_sentence = [\"The indicators tell me that this is not a good investment.\"]\nown_pred = model.predict(own_sentence)\n\nprint(own_pred)\npred_index = np.argmax(own_pred,axis = 1)\nprint(classes[pred_index])","metadata":{"execution":{"iopub.status.busy":"2023-03-27T20:50:14.050272Z","iopub.execute_input":"2023-03-27T20:50:14.051363Z","iopub.status.idle":"2023-03-27T20:50:14.138433Z","shell.execute_reply.started":"2023-03-27T20:50:14.051324Z","shell.execute_reply":"2023-03-27T20:50:14.137515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}